---
title: "`mcstate`"
author: "MRC Centre for Global Infectious Disease Analysis"
format: revealjs
---

# State space models

```{r}
r_output <- function(path, highlight = NULL) {
  if (is.null(highlight)) {
    prefix <- "```r"
  } else {
    prefix <- sprintf('```{.r code-line-numbers="%s"}', highlight)
  }
  writeLines(c(prefix, readLines(path), "```"))
}
set.seed(1) # always the same
knitr::knit_hooks$set(small_margins = function(before, options, envir) {
  if (before) {
    par(mar = c(4, 4, .1, .1))
  }
})
```

## What is it?

- A state space model (SSM) is a mathematical framework for modelling a dynamical system.
- It is built around two processes:
    - **state equations** that describes the evolution of some latent variables (also referred as "hidden" states) over time
    - **observation equations** that relates the observations to the latent variables.

## Can you be more precise?

![](images/SSM.jpg)

- $x_{t, 1 \leq t \leq T}$ the hidden states of the system 
- $y_{t, 1 \leq t \leq T}$ the observations
- $f_{\theta}$ the state transition function
- $g_{\theta}$ the observation function
- $t$ is often time
- $\theta$ defines the model

## Two common problems

![](images/SSM.jpg)

- Two common needs
  - "Filtering" i.e. estimate the hidden states $x_{t}$ from the observations $y_t$
  - "Inference" i.e. estimate the $\theta$'s compatible with the observations $y_{t}$

# (Bootstrap) Sequential Monte Carlo (the particle filter) {.smaller}

- Assuming a given $\theta$, at each time step $t$, BSSMC:
  1. generates $X_{t+1}^N$ by using $f_{\theta}(X_{t+1}^N|X_{t}^N)$ (the $N$ particles)
  2. calculates weights for the newly generated states based on $g_{\theta}(Y_{t+1}|X_{t+1})$
  3. resamples the states to keep only the good ones

- Allow to explores efficiently the state space by progressively integrating the data points

- Produces a MC approximation of $p(Y_{1:T}|\theta)$ the marginal likelihood

# Particle MCMC

## What is Particle MCMC? {.smaller}

- PMCMC is an algorithm which performs "filtering" and "inference"
- A Markov Chain Monte Carlo (MCMC) method for estimating target distributions
- MCMC explores the parameter space by moving randomly making jumps from one value to the next
- Probability of going from point to the other is determined by the proposal distribution and the ratio of the likelihood
- Compared with "traditional" MCMC, in PMCMC, the likelihood estimation is approximated using a "particle filter"
- The filter generates a set of "particles" i.e. trajectories compatible with the observation
- It uses these trajectories to compute a (marginal) likelihood that can be use by the PMCMC

## Core algorithm

1. **Initialisation** Start with a value $\theta_{0}$ from the parameter space
2. **Initial SMC** Use sequential Monte Carlo to do the "filtering" and samples of potential $\{X_{t}\}_{1..N}$. Calculate the (marginal) likelihood from this using a MC estimator
3. **Proposal** Propose a new parameter value $\theta ^*$
4. **SMC** Calculate marginal likelihood of proposal
5. **Metropolis-Hastings** Accept with probability $\min(1, \alpha)$ with $\alpha = \frac{p(\theta ^*)}{p(\theta_{t})} \cdot \frac{q(\theta_{t})}{q(\theta ^*)}$
6. **Loop** Redo (3) until the number of steps is reached

# mcstate

- `mcstate` is an R package based on the SSM paradigm which aims to provide inference and filtering tools for dust models
- Implements several main algorithms for this Particle MCMC (PMCMC), SMC^2, iterated filtering
- Inference tooling for the Centre's UK COVID model
- Subsequently used for other diseases

## Design philosophy

- Less well refined than odin/dust tbh
  - We may change and improve much of this, especially MCMC parameters
- More complex structures are built up from simpler objects
  - Filter {data, model, n_particles, compare}
  - PMCMC {parameters, filter, control}
- Provides you with low-level tools, and little handholding
- Pretty fast though

# Particle filtering

Our requirements

- A time series
- A generating model
- An index into model state
- A compare function

(for PMCMC you also need parameters to infer, ...later)

## The data

```{r}
#| echo: TRUE
#| small_margins: TRUE
incidence <- read.csv("data/incidence.csv")
head(incidence)
plot(cases ~ day, incidence, pch = 19, las = 1)
```

## Data preparation

```{r}
#| echo: TRUE
data <- mcstate::particle_filter_data(
  incidence, time = "day", rate = 4, initial_time = 0)
head(data)
```

## The model

```{r}
#| results: "asis"
r_output("models/sir.R")
```

## Compiling the model

```{r}
#| echo: TRUE
sir <- odin.dust::odin_dust("models/sir.R")
```

## The model over time

```{r}
#| echo: TRUE
#| small_margins: TRUE
#| code-line-numbers: "|4-5"
pars <- list(beta = 0.25, gamma = 0.1)
mod <- sir$new(pars, 0, 20)
y <- mod$simulate(c(0, data$time_end))
i <- mod$info()$index[["time"]]
j <- mod$info()$index[["cases_inc"]]
matplot(y[i, 1, ], t(y[j, , ]), type = "l", col = "#00000055", lty = 1, las = 1,
        xlab = "Day", ylab = "Cases")
points(cases ~ day, incidence, col = "red", pch = 19)
```

## The index function

* You rarely care about all the state variables
* You might want different state variables for your compare and for plotting

```{r}
#| echo: TRUE
index <- function(info) {
  list(run = c(incidence = info$index$cases_inc),
       state = c(t = info$index$time,
                 I = info$index$I,
                 cases = info$index$cases_inc))
}
index(mod$info())
```

## The compare function

![](images/SSM.jpg)

```{r}
#| echo: TRUE
compare <- function(state, observed, pars = NULL) {
  modelled <- state["incidence", , drop = TRUE]
  lambda <- modelled + rexp(length(modelled), 1e6)
  dpois(observed$cases, lambda, log = TRUE)
}
```

This is the important bit, and something that is a trick to write well.

## Files, from this repo

- [`incidence.csv`](data/incidence.csv) - daily case information
- [`sir.R`](models/sir.R) - a simple SIR model with incidence
- [`index.R`](R/index.R) - an index function
- [`compare.R`](R/compare.R) - a compare function

Or browse <https://github.com/mrc-ide/odin-dust-tutorial>

# A particle filter

```{r}
#| echo: TRUE
filter <- mcstate::particle_filter$new(data, model = sir, n_particles = 100,
                                       compare = compare, index = index)
pars <- list(beta = 0.25, gamma = 0.1)
filter$run(pars)
```

## Particle filter marginal likelihoods are stochastic!

```{r}
#| echo: TRUE
replicate(10, filter$run(pars))
```

## Likelihood variance changes with particle number

```{r}
#| echo: TRUE
filter <- mcstate::particle_filter$new(data, model = sir, n_particles = 10,
                                       compare = compare, index = index)
sort(replicate(10, filter$run(pars)))
filter <- mcstate::particle_filter$new(data, model = sir, n_particles = 1000,
                                       compare = compare, index = index)
sort(replicate(10, filter$run(pars)))
```

- Monte Carlo estimations typically see variance decrease with sample size, this is no different.
- You want a small variance, but that costs a lot of CPU time

## Likelihood mean changes with parameter values

```{r}
#| echo: TRUE
filter$run(list(beta = 0.2, gamma = 0.1))
filter$run(list(beta = 0.1, gamma = 0.05))
```

## Particle filter history

First, run the filter while saving history (off by default)

```{r}
#| echo: TRUE
filter <- mcstate::particle_filter$new(data, model = sir, n_particles = 100,
                                       compare = compare, index = index)
filter$run(save_history = TRUE)
```

## Particle filter history is a tree

```{r}
#| echo: TRUE
#| small_margins: TRUE
times <- data$time_end
h <- filter$history()
matplot(h["t", 1, ], t(h["cases", , ]), type = "l", col = "#00000011", 
        xlab = "Day", ylab = "Cases", las = 1)
points(cases ~ day, incidence, pch = 19, col = "red")
```

## Particle filter history for unobserved states

```{r}
#| echo: TRUE
#| small_margins: TRUE
matplot(h["t", 1, ], t(h["I", , ]), type = "l", col = "#00000011", 
        xlab = "Day", ylab = "Number of infecteds (I)", las = 1)
```

# PMCMC

- Particle MCMC - like MCMC but with a particle filter
- Slower, and harder to tune
- Easy to generate impossibly large amounts of data
- Inherits all the issues of MCMC that you know and love

## Algorithm

1. **Initialisation** Start with a value $\theta_{0}$ from the parameter space
2. **Initial SMC** Use sequential Monte Carlo to do the "filtering" and samples of potential $\{X_{t}\}_{1..N}$. Calculate the (marginal) likelihood from this using a MC estimator
3. **Proposal** Propose a new parameter value $\theta ^*$
4. **SMC** Calculate marginal likelihood of proposal
5. **Metropolis-Hastings** Accept with probability $\min(1, \alpha)$ with $\alpha = \frac{p(\theta ^*)}{p(\theta_{t})} \cdot \frac{q(\theta_{t})}{q(\theta ^*)}$
6. **Loop** Redo (3) until the number of steps is reached

## Defining your parameters

* Different to particle filter/model parameters
  - filter/model parameters are everything your model needs to run; may include data!
  - PMCMC parameters (often called $\theta$) are unstructured numeric vector
  - the PMCMC parameters are statistical parameters, your model parameters are functional parameters
* Requirements:
  - priors for MCMC parameters
  - proposal CV for multivariate normal
  - transformation from MCMC to model parameters

## Priors

```{r}
#| echo: TRUE
priors <- list(
  mcstate::pmcmc_parameter("beta", 0.2, min = 0),
  mcstate::pmcmc_parameter("gamma", 0.1, min = 0, prior = function(p)
    dgamma(p, shape = 1, scale = 0.2, log = TRUE)))
```

(this will improve in future, feedback very welcome)

## Proposal

* Variance covariance matrix for a multivariate normal distribution
* Symmetric (except for reflections at any provided boundaries)

```{r}
#| echo: TRUE
vcv <- diag(0.01, 2)
vcv
```

## Transformation

Convert "MCMC parameters" into "model parameters"

```{r}
#| echo: TRUE
transform <- function(theta) {
  as.list(theta)
}
```

You will want closures in complex models:

```r
make_transform <- function(contact_matrix, vaccine_schedule) {
  function(theta) {
    list(contact_matrix = contact_matrix,
         vaccine_schedule = vaccine_schedule,
         beta = theta[["beta"]],
         gamma = theta[["gamma"]])
  }
}
transform <- make_transform(contact_matrix, vaccine_schedule)
```

## Final parameter object

```{r}
#| echo: TRUE
mcmc_pars <- mcstate::pmcmc_parameters$new(priors, vcv, transform)
```

## Running PMCMC

```{r}
#| echo: TRUE
control <- mcstate::pmcmc_control(
    n_steps = 500,
    progress = TRUE)
samples <- mcstate::pmcmc(mcmc_pars, filter, control = control)
samples
plot(samples$probabilities[, "log_posterior"], type = "s",
     xlab = "Sample", ylab = "Log posterior")
```

---

oh.

## Assessing fit

* Just look at how bad it is
* In PMCMC, all your ideas from MCMC will be useful but can be misleading; i.e. adaptation is hard
* Gelman-Rubin convergence diagnostic
  - Run multiple chain
  - Check than within-chain variance is similar to between-chain variance
  - Necessary but not sufficient to prove convergence
* A lot of problems in MCMC come from autocorrelation
* Gelman Rubin diagnostic

## Autocorrelation

* Notion from time series, which translates for (P)MCMC in term of the steps of the chains
* Autocorrelation refers to the correlation between the values of a time series at different points in time. In MCMC, this means correlation between successive samples.
* In the context of MCMC, autocorrelation can most of the time be substituted instead of "bad mixing"
* A signature of random-walk MCMC
* Likely to bias estimate (wrong mean) and reduce variance compared with the true posterior distribution
* Linked with the notion of Effective Sample Size, roughly speaking ESS gives the equivalent in i.i.d. samples

## Autocorrelation in practice FAQ

* **Why is Autocorrelation a Problem?** For optimal performance, we want the samples to be independent and identically distributed (i.i.d.) samples from the target distribution. 
* **How to Detect Autocorrelation?** We can calculate the **autocorrelation function (ACF)**, which measures the correlation between the samples and their lagged values.
* **How to Reduce Autocorrelation?** To mitigate the problem of autocorrelation, there's a number of strategies, including: using a longer chain, adapting the proposal distribution, using thinning or subsampling techniques. By reducing autocorrelation, we can obtain better estimates of the target distribution and improve the accuracy of our Bayesian inference.

## Thinning the chain

* Either before or after fit
* Faster and less memory to thin before
* More flexible to thin later
* No real difference if history not saved

This is useful because most of your chain is not interesting due to the autocorrelation.

## Running in parallel

Arguments to  [`mcstate::pmcmc_control`](https://mrc-ide.github.io/mcstate/reference/pmcmc_control.html)

* `n_chains`: number of separate chains to run
* `n_threads_total`: total number of threads to use
* `n_workers`: number of separate threads to split your chains over
* `use_parallel_seed`: helps with reproducibility

You can also run different chains on different cluster nodes - but talk to us about this.

## Let's try again

```{r}
vcv <- matrix(c(0.00057, 0.00052, 0.00052, 0.00057), 2, 2)
mcmc_pars <- mcstate::pmcmc_parameters$new(priors, vcv, transform)
control <- mcstate::pmcmc_control(
    n_steps = 500,
    n_chains = 4,
    n_threads_total = 12,
    n_workers = 4,
    save_state = TRUE,
    save_trajectories = TRUE,
    progress = TRUE)
samples <- mcstate::pmcmc(mcmc_pars, filter, control = control)
plot(samples$probabilities[, "log_posterior"], type = "s")
```

## Saving history

* Save your trajectories at every collected sample
* Save the final state at every sample
* Save full model state at specific points.

# Next steps

* forward time predictions
* posterior predictive checks
* closures and binding data into functions
* min log likelihood (and filter early exit)
* rerun filter in mcmc

# Advanced topics

* compiled compare functions
* multi-parameter models
* multi-stage models
* restarting models
* deterministic (expectation) models as starting points
* adaptive fitting (deterministic models only)
* use on a GPU
* use with ODE/SDE models
* other inference methods - if2, smc2

# Resources {.smaller}

A nice PMCMC introduction written for the epidemiologist
[Endo, A., van Leeuwen, E. & Baguelin, M. Introduction to particle Markov-chain Monte Carlo for disease dynamics modellers. Epidemics 29, 100363 (2019).] (https://www.sciencedirect.com/science/article/pii/S1755436519300301?via%3Dihub)

A tutorial about SMC
[Doucet, A. & Johansen, A. M. A Tutorial on Particle filtering and smoothing: Fiteen years later. Oxford Handb. nonlinear Filter. 656–705 (2011). doi:10.1.1.157.772](https://www.stats.ox.ac.uk/~doucet/doucet_johansen_tutorialPF2011.pdf)

The reference paper on PMCMC
[Andrieu, C., Doucet, A. & Holenstein, R. Particle Markov chain Monte Carlo methods. J. R. Stat. Soc. Ser. B (Statistical Methodol. 72, 269–342 (2010).](https://www.stats.ox.ac.uk/~doucet/andrieu_doucet_holenstein_PMCMC.pdf)

A software oriented paper introducing odin, dust and mcstate
[R. G. FitzJohn et al. Reproducible parallel inference and simulation of stochastic state space models using odin, dust, and mcstate. Wellcome Open Res. 2021 5288 5, 288 (2021).](https://wellcomeopenresearch.org/articles/5-288)

<!--  LocalWords:  mcstate revealjs sprintf writeLines readLines SSM
 -->
<!--  LocalWords:  envir leq BSSMC Resample propto MCMC PMCMC SMC tbh
 -->
<!--  LocalWords:  frac cdot handholding csv pch las asis matplot lty
 -->
<!--  LocalWords:  xlab ylab rexp dpois repo infecteds dgamma vcv ESS
 -->
<!--  LocalWords:  diag mcmc Gelman SDE
 -->
