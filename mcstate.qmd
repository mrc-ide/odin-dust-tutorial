---
title: "`mcstate`"
author: "MRC Centre for Global Infectious Disease Analysis"
format: revealjs
---

# State space models

```{r}
r_output <- function(path, highlight = NULL) {
  if (is.null(highlight)) {
    prefix <- "```r"
  } else {
    prefix <- sprintf('```{.r code-line-numbers="%s"}', highlight)
  }
  writeLines(c(prefix, readLines(path), "```"))
}
set.seed(1) # always the same
```

## What is it?

- A state space model (SSM) is a mathematical framework for modeling a dynamical system.
- It is built around two processes:
    - **state equations** that describes the evolution of some latent variables (also referred as "hidden" states) over time
    - **observation equations** that relates the observations to the latent variables.

## A diagram

![State space model diagram](images/SSM.jpg)

- $x_{t, 1 \leq t \leq T}$ the hidden states of the system 
- $y_{t, 1 \leq t \leq T}$ the observations
- $f_{\theta}$ the state transition function
- $g_{\theta}$ the observation function
- $t$ is often time
- $\theta$ defines the model

## Two common problems

![](images/SSM.jpg)

- Two common needs
  - "Filtering" i.e. estimate the hidden states $x_{t}$ from the observations $y_t$
  - "Inference" i.e. estimate the $\theta$'s compatible with the observations $y_{t}$

# Particle MCMC

## What is Particle MCMC?

- PMCMC is an algorithm which performs "filtering" and "inference"
- A Markov Chain Monte Carlo (MCMC) method for estimating target distributions
- MCMC explores the parameter space by moving randomly making jumps from one value to the next
- Probability of going from point to the other is determined by the proposal distribution and the ratio of the likelihood
- Compared with "traditional" MCMC, in PMCMC, the likelihood estimation is approximated using a "particle filter"
- The filter generates a set of "particles" i.e. trajectories compatible with the observation
- It uses these trajectories to compute a (marginal) likelihood that can be use by the PMCMC

## Core algorithm

1. **Initialisation** Start with a value $\theta_{0}$ from the parameter space
2. **Initial SMC** Use sequential Monte Carlo to do the "filtering" and samples of potential $\{X_{t}\}_{1..N}$. Calculate the (marginal) likelihood from this using a MC estimator
3. **Proposal** Propose a new parameter value $\theta ^*$
4. **SMC** Calculate marginal likelihood of proposal
5. **Metropolis-Hastings** Accept with probability $\min(1, \alpha)$ with $\alpha = \frac{p(\theta ^*)}{p(\theta_{t})} \cdot \frac{q(\theta_{t})}{q(\theta ^*)}$
6. **Loop** Redo (3) until the number of steps is reached

# mcstate

- mcstate is an R package based on the SSM paradigm which aims to provide inference and filtering tools for dust models
- it implements several main algorithms for this Particle MCMC (PMCMC), SMC^2, iterated filtering
- the inference tooling for the Centre's UK covid model
- Subsequently used for mpox, gonorhea, strep pneumo, malaria...

## Design philosophy

- Less well refined than odin/dust tbh
  - We may change and improve much of this, especially mcmc parameters
- more complex structures are built up from simpler objects
  - Filter {data, model, n_particles, compare}
  - PMCMC {parameters, filter, control}
- provides you with low-level tools, and little handholding
- pretty fast though

# Particle filtering

Our requirements

- A time series
- A generating model
- An index into model state
- A compare function

(for PMCMC you also need parameters to infer, ...later)

## data

```{r}
#| echo = TRUE
incidence <- read.csv("data/incidence.csv")
head(incidence)
plot(cases ~ day, incidence, pch = 19, las = 1)
```

```{r}
#| echo = TRUE
data <- mcstate::particle_filter_data(incidence, time = "day", rate = 4, initial_time = 0)
head(data)
```

## model

```{r}
#| results: "asis"
r_output("models/sir.R")
```

or

```{r}
#| echo = TRUE
sir <- odin.dust::odin_dust("models/sir.R")
```

## The model over time

```{r}
#| echo = TRUE
pars <- list(beta = 0.25, gamma = 0.1)
mod <- sir$new(pars, 0, 20)
y <- mod$simulate(c(0, data$time_end))
i <- mod$info()$index[["time"]]
j <- mod$info()$index[["cases_inc"]]
matplot(y[i, 1, ], t(y[j, , ]), type = "l", col = "#00000055", lty = 1, las = 1,
        xlab = "Day", ylab = "Cases")
points(cases ~ day, incidence, col = "red", pch = 19)
```

## index function

* You rarely care about all variables
* You might want different variables for your compare and for plotting

```{r}
#| echo = TRUE
index <- function(info) {
  list(run = c(incidence = info$index$cases_inc),
       state = c(t = info$index$time,
                 I = info$index$I,
                 cases = info$index$cases_inc))
}
index(mod$info())
```

## compare

![State space model diagram](images/SSM.jpg)

```{r}
#| echo = TRUE
compare <- function(state, observed, pars = NULL) {
  modelled <- state["incidence", , drop = TRUE]
  lambda <- modelled + rexp(length(modelled), 1e6)
  dpois(observed$cases, lambda, log = TRUE)
}
```

given

```{r}
#| echo = TRUE
index
head(data)
```

This is the important bit, and something that is a trick to write well.

## Files, from this repo

- [`incidence.csv`](data/incidence.csv) - daily case information
- [`sir.R`](models/sir.R) - a simple SIR model with incidence
- [`index.R`](R/index.R) - an index function
- [`compare.R`](R/compare.R) - a compare function

Or browse <https://github.com/mrc-ide/odin-dust-tutorial>

# A particle filter

```{r}
#| echo = TRUE
filter <- mcstate::particle_filter$new(data, model = sir, n_particles = 100,
                                       compare = compare, index = index)
pars <- list(beta = 0.25, gamma = 0.1)
filter$run(pars)
```

## Particle filter marginal likelihoods are stochastic!

```{r}
#| echo = TRUE
replicate(10, filter$run(pars))
```

## Likelihood variance changes with particle number

```{r}
#| echo = TRUE
filter <- mcstate::particle_filter$new(data, model = sir, n_particles = 10,
                                       compare = compare, index = index)
sort(replicate(10, filter$run(pars)))
filter <- mcstate::particle_filter$new(data, model = sir, n_particles = 1000,
                                       compare = compare, index = index)
sort(replicate(10, filter$run(pars)))
```

- Monte Carlo estimations typically see variance decrease with sample size, this is no different.
- You want a small variance, but that costs a lot of CPU time

## Likelihood mean changes with parameter values

[...]

## Particle filter history is a tree

```{r}
#| echo = TRUE
filter <- mcstate::particle_filter$new(data, model = sir, n_particles = 100,
                                       compare = compare, index = index)
filter$run(save_history = TRUE)
times <- data$time_end
h <- filter$history()
matplot(h["t", 1, ], t(h["cases", , ]), type = "l", col = "#00000011", 
        xlab = "Day", ylab = "Cases", las = 1)
points(cases ~ day, incidence, pch = 19, col = "red")
```

```{r}
#| echo = TRUE
matplot(h["t", 1, ], t(h["I", , ]), type = "l", col = "#00000011", 
        xlab = "Day", ylab = "Number of infecteds (I)", las = 1)
```

# PMCMC

- Particle MCMC - like MCMC but with a particle filter
- Slower, and harder to tune
- Easy to generate impossibly large amounts of data
- Inherits all the issues of MCMC that you know and love

## Algorithm

1. **Initialisation** Start with a value $\theta_{0}$ from the parameter space
2. **Initial SMC** Use sequential Monte Carlo to do the "filtering" and samples of potential $\{X_{t}\}_{1..N}$. Calculate the (marginal) likelihood from this using a MC estimator
3. **Proposal** Propose a new parameter value $\theta ^*$
4. **SMC** Calculate marginal likelihood of proposal
5. **Metropolis-Hastings** Accept with probability $\min(1, \alpha)$ with $\alpha = \frac{p(\theta ^*)}{p(\theta_{t})} \cdot \frac{q(\theta_{t})}{q(\theta ^*)}$
6. **Loop** Redo (3) until the number of steps is reached

## Defining your parameters

* Different to particle filter/model parameters
  - filter/model parameters are everything your model needs to run; may include data!
  - pmcmc parameters (often called $\theta$) are unstructured numeric vector
  - the pmcmc parameters are statistical parameters, your model parameters are functional parameters
* Requirements:
  - priors for mcmc parameters
  - proposal vcv for multivariate normal
  - transformation from mcmc to model parameters

## Priors

```{r}
#| echo = TRUE
priors <- list(
  mcstate::pmcmc_parameter("beta", 0.2, min = 0),
  mcstate::pmcmc_parameter("gamma", 0.1, min = 0, prior = function(p)
    dgamma(p, shape = 1, scale = 0.2, log = TRUE)))
```

(this will improve in future, feedback very welcome)

## Proposal

* Variance covariance matrix for a multivariate normal distribution
* Symmetric (except for reflections at any provided boundaries)

```{r}
#| echo = TRUE
vcv <- diag(0.1, 2)
vcv
```

## Transformation

Convert "mcmc parameters" into "model parameters"

```{r}
#| echo = TRUE
transform <- function(theta) {
  as.list(theta)
}
```

You will want closures in complex models:

```r
make_transform <- function(contact_matrix, vaccine_schedule) {
  function(theta) {
    list(contact_matrix = contaxt_matrix,
         vaccine_schedule = vaccine_schedule,
         beta = theta[["beta"]],
         gamma = theta[["gamma"]])
  }
}
transform <- make_transform(contact_matrix, vaccine_schedule)
```

## Final parameter object

```{r}
#| echo = TRUE
mcmc_pars <- mcstate::pmcmc_parameters$new(priors, vcv, transform)
```

## Running PMCMC

```{r}
#| echo = TRUE
control <- mcstate::pmcmc_control(
    n_steps = 100,
    save_state = TRUE,
    save_trajectories = TRUE,
    progress = TRUE)
samples <- mcstate::pmcmc(mcmc_pars, filter, control = control)
samples
```

## Assessing fit

* Just look at how bad it is
* In PMCMC, all your ideas from MCMC will be useful but can be misleading; i.e. adaptation is hard
* Gelman's Rubin diagnostic
* A lot of problems in MCMC come from autocorrelation

## Autocorrelation

* Notion from time series, which translates for (P)MCMC in term of the steps of the chains
* Autocorrelation refers to the correlation between the values of a time series at different points in time. In MCMC, this means correlation between successive samples.
* In the context of MCMC, autocorrelation can most of the time be substituted instead of "bad mixing"
* A signature of random-walk MCMC
* Likely to bias estimate (wrong mean) and reduce variance compare with the true posterior distribution of the estimate 
* Linked with the notion of Effective Sample Size, roughly speaking ESS gives the equivalent in i.i.d. samples

## Autocorrelation in practice FAQ

* **Why is Autocorrelation a Problem?** For optimal performance, we want the samples to be independent and identically distributed (i.i.d.) samples from the target distribution. However, if the Markov Chain has high autocorrelation, it means that the samples are not i.i.d., and they do not accurately represent the target distribution.
* **How to Detect Autocorrelation?** We can calculate the **autocorrelation function (ACF)**, which measures the correlation between the samples and their lagged values.
* **How to Reduce Autocorrelation?** To mitigate the problem of autocorrelation, there's a number of strategies, including: using a longer chain, adapting the proposal distribution, using thinning or subsampling techniques. By reducing autocorrelation, we can obtain better estimates of the target distribution and improve the accuracy of our Bayesian inference.

## Thinning the chain

* Either before or after fit
* Faster and less memory to thin before
* More flexible to thin later
* No real difference if history not saved

This is useful because most of your chain is not interesting due to the autocorrelation.

## Plotting your chains

```{r}
#| echo = TRUE
samples$probabilities[, "log_posterior"]
```

## Saving history

* Save your trajectories at every collected sample
* Save the final state at every sample
* Save full model state at specific points.

# Intermediate topics

* forward time predictions
* posterior predictive checks
* closures and binding data into functions
* running in parallel - chains and particles
* min log likelihood (and filter early exit)
* rerun filter in mcmc

# Advanced topics

* compiled compare functions
* multi-parameter models
* multi-stage models
* restarting models
* deterministic (expectation) models as starting points
* adaptive fitting (deterministic models only)
* use on a GPU
* use with ODE/SDE models
* other inference methods - if2, smc2

# Resources

A nice PMCMC introduction written for the epidemiologist
[Endo, A., van Leeuwen, E. & Baguelin, M. Introduction to particle Markov-chain Monte Carlo for disease dynamics modellers. Epidemics 29, 100363 (2019).] (https://www.sciencedirect.com/science/article/pii/S1755436519300301?via%3Dihub)

A tutorial about SMC
[Doucet, A. & Johansen, A. M. A Tutorial on Particle filtering and smoothing: Fiteen years later. Oxford Handb. nonlinear Filter. 656–705 (2011). doi:10.1.1.157.772](https://www.stats.ox.ac.uk/~doucet/doucet_johansen_tutorialPF2011.pdf)

The reference paper on PMCMC
[Andrieu, C., Doucet, A. & Holenstein, R. Particle Markov chain Monte Carlo methods. J. R. Stat. Soc. Ser. B (Statistical Methodol. 72, 269–342 (2010).](https://www.stats.ox.ac.uk/~doucet/andrieu_doucet_holenstein_PMCMC.pdf)

A software oriented paper introducing odin, dust and mcstate
[R. G. FitzJohn et al. Reproducible parallel inference and simulation of stochastic state space models using odin, dust, and mcstate. Wellcome Open Res. 2021 5288 5, 288 (2021).](https://wellcomeopenresearch.org/articles/5-288)






